{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3704981c-5fd3-4f01-a2cc-c54920a97977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "access_key = os.getenv(\"AWS_ACCESS_KEY_ID\", \"Не задано\")\n",
    "secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\", \"Не задано\")\n",
    "s3_endpoint = os.getenv(\"AWS_S3_ENDPOINT\", \"Не задано\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7982bd9e-0ecb-4da5-aef5-61f5176672fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/12 12:18:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "[Stage 0:>                                                        (0 + 12) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Создание SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BostonCrimeAnalysis\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", access_key) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", secret_key) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", s3_endpoint) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.maximum\", \"100\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")  \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.region\", \"us-east-1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Тестовый запрос\n",
    "spark.range(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e3d6b89-299c-4932-b758-3466db6a7bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- INCIDENT_NUMBER: string (nullable = true)\n",
      " |-- OFFENSE_CODE: integer (nullable = true)\n",
      " |-- OFFENSE_CODE_GROUP: string (nullable = true)\n",
      " |-- OFFENSE_DESCRIPTION: string (nullable = true)\n",
      " |-- DISTRICT: string (nullable = true)\n",
      " |-- REPORTING_AREA: string (nullable = true)\n",
      " |-- SHOOTING: string (nullable = true)\n",
      " |-- OCCURRED_ON_DATE: string (nullable = true)\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- DAY_OF_WEEK: string (nullable = true)\n",
      " |-- HOUR: integer (nullable = true)\n",
      " |-- UCR_PART: string (nullable = true)\n",
      " |-- STREET: string (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      "\n",
      "+---------------+------------+--------------------+--------------------+--------+--------------+--------+-------------------+----+-----+-----------+----+----------+-----------+-----------+------------+--------------------+\n",
      "|INCIDENT_NUMBER|OFFENSE_CODE|  OFFENSE_CODE_GROUP| OFFENSE_DESCRIPTION|DISTRICT|REPORTING_AREA|SHOOTING|   OCCURRED_ON_DATE|YEAR|MONTH|DAY_OF_WEEK|HOUR|  UCR_PART|     STREET|        Lat|        Long|            Location|\n",
      "+---------------+------------+--------------------+--------------------+--------+--------------+--------+-------------------+----+-----+-----------+----+----------+-----------+-----------+------------+--------------------+\n",
      "|     I182070945|         619|             Larceny|  LARCENY ALL OTHERS|     D14|           808|    null|2018-09-02 13:00:00|2018|    9|     Sunday|  13|  Part One| LINCOLN ST|42.35779134|-71.13937053|(42.35779134, -71...|\n",
      "|     I182070943|        1402|           Vandalism|           VANDALISM|     C11|           347|    null|2018-08-21 00:00:00|2018|    8|    Tuesday|   0|  Part Two|   HECLA ST|42.30682138|-71.06030035|(42.30682138, -71...|\n",
      "|     I182070941|        3410|               Towed| TOWED MOTOR VEHICLE|      D4|           151|    null|2018-09-03 19:27:00|2018|    9|     Monday|  19|Part Three|CAZENOVE ST|42.34658879|-71.07242943|(42.34658879, -71...|\n",
      "|     I182070940|        3114|Investigate Property|INVESTIGATE PROPERTY|      D4|           272|    null|2018-09-03 21:16:00|2018|    9|     Monday|  21|Part Three| NEWCOMB ST|42.33418175|-71.07866441|(42.33418175, -71...|\n",
      "|     I182070938|        3114|Investigate Property|INVESTIGATE PROPERTY|      B3|           421|    null|2018-09-03 21:05:00|2018|    9|     Monday|  21|Part Three|   DELHI ST|42.27536542|-71.09036101|(42.27536542, -71...|\n",
      "+---------------+------------+--------------------+--------------------+--------+--------------+--------+-------------------+----+-----+-----------+----+----------+-----------+-----------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Загрузка данных\n",
    "input_path = '/data/crime.csv'\n",
    "crimes_df = spark.read.option(\"header\", \"true\").csv(input_path, inferSchema=True)\n",
    "crimes_df.printSchema()\n",
    "crimes_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4006ab10-fe02-4532-a2ef-743184ec02ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from functools import reduce  # Добавлен импорт reduce\n",
    "\n",
    "def clean_boston_crime_data(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Очищает данные о преступлениях в Бостоне:\n",
    "    - Заполняет пропуски в столбце 'SHOOTING'.\n",
    "    - Преобразует 'OCCURRED_ON_DATE' в формат timestamp.\n",
    "    - Разделяет 'Location' на 'Lat' и 'Long'.\n",
    "    - Проверяет допустимость значений координат.\n",
    "    - Удаляет дубликаты по 'INCIDENT_NUMBER'.\n",
    "    - Проверяет и фильтрует значения в 'UCR_PART'.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): Исходный DataFrame с данными о преступлениях.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: Очищенный DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Заполнение пропусков в столбце SHOOTING значением False\n",
    "    df_cleaned = df.fillna({'SHOOTING': 'False'})\n",
    "    \n",
    "    # Преобразование столбца OCCURRED_ON_DATE в правильный формат даты\n",
    "    df_cleaned = df_cleaned.withColumn(\"OCCURRED_ON_DATE\", F.to_timestamp(\"OCCURRED_ON_DATE\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    \n",
    "    # Разделение Location на Lat и Long\n",
    "    df_cleaned = df_cleaned.withColumn(\"Lat\", F.regexp_extract(\"Location\", r\"\\((.*),\", 1).cast(\"double\")) \\\n",
    "                           .withColumn(\"Long\", F.regexp_extract(\"Location\", r\", (.*)\\)\", 1).cast(\"double\"))\n",
    "    \n",
    "    # Проверка значений Lat и Long (широта от -90 до 90, долгота от -180 до 180)\n",
    "    df_cleaned = df_cleaned.filter((df_cleaned.Lat >= -90) & (df_cleaned.Lat <= 90) &\n",
    "                                   (df_cleaned.Long >= -180) & (df_cleaned.Long <= 180))\n",
    "    \n",
    "    # Удаление дубликатов по INCIDENT_NUMBER\n",
    "    df_cleaned = df_cleaned.dropDuplicates(subset=[\"INCIDENT_NUMBER\"])\n",
    "    \n",
    "    # Проверка на корректность значений в столбце UCR_PART (должны быть только \"Part One\", \"Part Two\", \"Part Three\", \"Other\")\n",
    "    valid_ucr_parts = [\"Part One\", \"Part Two\", \"Part Three\", \"Other\"]\n",
    "    df_cleaned = df_cleaned.filter(df_cleaned.UCR_PART.isin(valid_ucr_parts))\n",
    "\n",
    "    # Столбцы для проверки на None\n",
    "    columns_to_check = ['UCR_PART', 'Lat', 'Long', 'DISTRICT']\n",
    "\n",
    "    # Формирование условия для фильтрации строк с None в указанных столбцах\n",
    "    condition = reduce(lambda acc, col: acc & df[col].isNotNull(), columns_to_check, F.lit(True))\n",
    "\n",
    "    # Применение фильтрации\n",
    "    df_cleaned = df_cleaned.filter(condition)\n",
    "    \n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c7c5c63-6ed5-4b89-a45e-451b3caf0c1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_boston_crime_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_cleaned \u001b[38;5;241m=\u001b[39m \u001b[43mclean_boston_crime_data\u001b[49m(crimes_df)\n\u001b[1;32m      2\u001b[0m df_cleaned\u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clean_boston_crime_data' is not defined"
     ]
    }
   ],
   "source": [
    "df_cleaned = clean_boston_crime_data(crimes_df)\n",
    "df_cleaned.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf70fb54-da2e-42c0-b199-07585a1bdc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "def create_bi_dataset(df):\n",
    "    # Извлечение первой части названия преступления (crime_type) из столбца 'OFFENSE_DESCRIPTION'\n",
    "    df = df.withColumn('crime_type', F.split(df['OFFENSE_DESCRIPTION'], ' - ')[0])\n",
    "    \n",
    "    # Рассчитываем общее количество преступлений по районам\n",
    "    crimes_total = df.groupBy('DISTRICT').agg(F.count('*').alias('crimes_total'))\n",
    "    \n",
    "    # Медиана числа преступлений в месяц в каждом районе\n",
    "    crimes_monthly = df.groupBy('DISTRICT', 'YEAR', 'MONTH').agg(F.count('*').alias('monthly_crimes'))\n",
    "    crimes_monthly = crimes_monthly.groupBy('DISTRICT').agg(F.expr('percentile_approx(monthly_crimes, 0.5)').alias('crimes_monthly'))\n",
    "    \n",
    "    # Три самых частых типа преступлений по району\n",
    "    frequent_crime_types = df.groupBy('DISTRICT', 'crime_type').agg(F.count('*').alias('crime_type_count'))\n",
    "    frequent_crime_types = frequent_crime_types.withColumn('rank', F.row_number().over(Window.partitionBy('DISTRICT').orderBy(F.desc('crime_type_count'))))\n",
    "    frequent_crime_types = frequent_crime_types.filter(frequent_crime_types['rank'] <= 3)\n",
    "    frequent_crime_types = frequent_crime_types.groupBy('DISTRICT').agg(F.concat_ws(', ', F.collect_list('crime_type')).alias('frequent_crime_types'))\n",
    "    \n",
    "    # Средняя широта и долгота для района\n",
    "    avg_coordinates = df.groupBy('DISTRICT').agg(\n",
    "        F.avg('Lat').alias('lat'),\n",
    "        F.avg('Long').alias('lng')\n",
    "    )\n",
    "    \n",
    "    # Объединение всех агрегаций в одну витрину\n",
    "    dashboard = crimes_total.join(crimes_monthly, on='DISTRICT', how='inner') \\\n",
    "                            .join(frequent_crime_types, on='DISTRICT', how='inner') \\\n",
    "                            .join(avg_coordinates, on='DISTRICT', how='inner')\n",
    "\n",
    "    return dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48cda6cd-3356-48a3-a846-30cae2751b7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_cleaned' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Создание витрины\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m bi_dataset \u001b[38;5;241m=\u001b[39m create_bi_dataset(\u001b[43mdf_cleaned\u001b[49m)\n\u001b[1;32m      3\u001b[0m bi_dataset\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_cleaned' is not defined"
     ]
    }
   ],
   "source": [
    "# Создание витрины\n",
    "bi_dataset = create_bi_dataset(df_cleaned)\n",
    "bi_dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45a906a9-bed7-4d38-9655-b0764728cb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred while saving to MinIO: name 'bi_dataset' is not defined\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    path = \"output/bi_data.parquet\"\n",
    "    bucket_name = \"spark-data\"\n",
    "    output_path = f\"s3a://{bucket_name}/{path}\"\n",
    "    bi_dataset.write.parquet(output_path)\n",
    "    print(f\"Data successfully uploaded to {output_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred while saving to MinIO: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2276e8-fd37-4e13-aff7-c3237d8c8383",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
